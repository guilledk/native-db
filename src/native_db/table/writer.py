import os

from io import BytesIO
from itertools import count

from pathlib import Path
from logging import Logger, getLogger
from typing import TYPE_CHECKING

import msgspec
import polars as pl

from native_db.schema import Column
from native_db.table._layout import MonoPartOptions

if TYPE_CHECKING:
    from native_db.table import Table


log = getLogger(__name__)


class StagedFrame(msgspec.Struct):
    index: int
    number_of_rows: int
    path: Path

    def scan(self) -> pl.LazyFrame:
        return pl.scan_ipc(self.path)


class TableWriter:
    '''
    # Overview

    Helper for stream-writing tables larger than memory while respecting the
    semantics emposed by the hint system (check `native_db.dtypes` module
    docstring for information on hint semantics).

    Only a single writer is allowed per table directory.

    IMPORTANT: Don't call `.push/push_ipc` from different threads.

    # Staging vs Final

    Before doing any processing, partitioning or ordering the first step is to
    store any received frame in the "staging area" which is a subdirectory on
    the table's local path.

    The writer will keep an in memory representation of the frames in staging
    area, and once reaching a threshold it will perform all operations needed
    to join the frames respecting the table semantics specified through the
    hint system.

    Then the frame has to be moved to the "final area" for long term storage,
    which might also involve additional operations in order to arrange the
    previous frames in final area to include this new frame, keeping all
    hint semantics which are crucial for query speed later.

    ## Staging area files

    The name of the file will encode information like the number of rows and
    frame index.

    Example:

        `table_path/.staging/frame-0001.10000-rows.ipc`

        frame index: 1
        number of rows: 10k

    '''
    def __init__(
        self,
        table: 'Table',
        *,
        log: Logger  = log
    ):
        self.log = log
        self._table = table
        self.layout = table.layout

        # staging state tracking

        # on disk path
        self._staging_path = table.local_path / '.staging'
        # in-memory representation of current staging area
        self._staging: list[StagedFrame] = []
        # rows stored in staging
        self._staged_rows: int = 0

        # when push is gonna be called with in order frames, an autogenerated
        # frame index is used provided by itertools.count
        self._frame_index: count[int] = count(self.layout.start_frame_index)

        # number of rows to store in staging area before a commit
        self._commit_threshold: int = self.layout.commit_threshold

        # if set attempt to partition final area by this column
        self._partition_column: Column | None = (
            None
            if not self.layout.partitioning
            else
            self.layout.partitioning.column(self._table.schema)
        )

        # rows already committed as full parts
        self._commited_rows: int = self._discover_committed_rows()

        # next parquet part index in the final dir
        self._next_part_index: int = self._discover_next_part_index()

        # next expected frame index for gap-aware commit
        self._next_expected_index: int = self.layout.start_frame_index

    def _parquet_parts(self) -> tuple[Path, ...]:
        if not self._table.local_path.is_dir():
            return tuple()

        return tuple(sorted(self._table.local_path.rglob('**/part-*.parquet')))

    def _discover_next_part_index(self) -> int:
        '''
        Scan the final directory and return the next 'part-xxxxx.parquet' index
        to use. If no files are present, return 0.

        '''
        max_idx = -1
        for p in self._parquet_parts():
            name = p.name  # e.g. 'part-00012.parquet'
            try:
                idx_str = name.split('-')[1].split('.')[0]
                idx = int(idx_str)
            except Exception:
                continue

            if idx > max_idx:
                max_idx = idx

        return max_idx + 1

    def _discover_committed_rows(self) -> int:
        '''
        Count how many full parts are present anywhere under final dir. Each
        part has exactly rows_per_file rows.

        '''
        if not self._table.local_path.is_dir():
            return 0

        return len(self._parquet_parts()) * self.layout.rows_per_file

    def is_commit_required(self) -> bool:
        return self._staged_rows >= self._commit_threshold

    def _final_path(self) -> Path:
        if not self._partition_column:
            return self._table.local_path

        assert self.layout.partitioning
        return self.layout.partitioning.partition(self._table.local_path, self._commited_rows)

    def _maybe_commit(self) -> None:
        if not self._staging or not self.is_commit_required():
            return

        # order by frame index and find the longest contiguous prefix
        staged = sorted(self._staging, key=lambda s: s.index)
        if not staged:
            return

        expected = self._next_expected_index

        if staged[0].index != expected:
            # gap at the very front: cannot commit anything yet
            return

        expected += 1
        prefix: list[StagedFrame] = [staged[0]]
        for sf in staged[1:]:
            # require strict consecutiveness
            if sf.index != expected:
                break
            prefix.append(sf)
            expected += 1

        # only the contiguous prefix participates in the commit decision
        total_rows = sum(sf.number_of_rows for sf in prefix)
        if total_rows < self.layout.rows_per_file:
            return

        # lazy concat into single frame
        staging_frames = [pl.scan_ipc(sf.path) for sf in prefix]
        staging_frame = pl.concat(staging_frames, rechunk=False)

        full_rows = (total_rows // self.layout.rows_per_file) * self.layout.rows_per_file
        n_parts = full_rows // self.layout.rows_per_file

        # write exact-sized parquet parts (in order)
        offset: int = 0
        for i in range(n_parts):
            lf = staging_frame.slice(offset, self.layout.rows_per_file)

            out = self._final_path() / f'{self._table.prefix}-part-{(self._next_part_index + i):05}.{self._table.suffix}'
            lf.sink_parquet(
                out,
                compression=self._table.compression,
                compression_level=self._table.compression_level,
                row_group_size=self._table.schema.row_group_size,
                statistics=True,
                mkdir=True,
            )
            offset += self.layout.rows_per_file
            self._commited_rows += self.layout.rows_per_file

        self._next_part_index += n_parts

        # Rebuild staging:
        # - consume from the contiguous prefix only (front to back)
        # - leave post-gap frames untouched
        to_consume = full_rows
        new_staging: list[StagedFrame] = []

        fully_consumed_frames = 0
        for sf in prefix:
            if to_consume == 0:
                new_staging.append(sf)
                continue

            if to_consume >= sf.number_of_rows:
                # fully consumed -> delete file
                try:
                    sf.path.unlink(missing_ok=True)
                except Exception:
                    pass
                to_consume -= sf.number_of_rows
                fully_consumed_frames += 1
                continue

            # partially consumed -> trim leading rows and keep remainder
            keep_rows = sf.number_of_rows - to_consume
            tmp = sf.path.with_suffix(sf.path.suffix + '.tmp')
            (
                pl.scan_ipc(sf.path)
                .slice(to_consume, keep_rows)
                .sink_ipc(tmp, compression='uncompressed')
            )
            os.replace(tmp, sf.path)
            to_consume = 0
            new_staging.append(
                StagedFrame(index=sf.index, number_of_rows=keep_rows, path=sf.path)
            )

        # append untouched (post-gap) frames as-is
        seen = {sf.index for sf in prefix}
        for sf in staged:
            if sf.index not in seen:
                new_staging.append(sf)

        # swap in rebuilt staging and recompute counters
        self._staging = sorted(new_staging, key=lambda s: s.index)
        self._staged_rows = sum(s.number_of_rows for s in self._staging)

        # advance expected frame index by the number of fully-consumed frames
        # (if the last prefix frame was partial, its index remains as the next expected)
        self._next_expected_index += fully_consumed_frames

    def push(
        self,
        frame: pl.LazyFrame | pl.DataFrame,
        *,
        frame_index: int | None = None,
        number_of_rows: int = 0
    ) -> None:
        # ensure lazy frame
        if isinstance(frame, pl.DataFrame):
            frame = frame.lazy()

        # generate next frame's properties
        index = next(self._frame_index) if frame_index is None else frame_index

        if not number_of_rows:
            number_of_rows = frame.select(pl.len()).collect().item()

        path = self._staging_path / f'frame-{index:04}.{number_of_rows}-rows.ipc'

        # write as ipc
        frame.sink_ipc(path, compression='uncompressed', mkdir=True)

        # add to staging frame list
        self._staging.append(StagedFrame(
            index=index,
            number_of_rows=number_of_rows,
            path=path
        ))

        self._staged_rows += number_of_rows

        self._maybe_commit()

    def push_ipc(self, payload: bytes | memoryview, **kwargs) -> None:
        # to lazy frame view
        frame = pl.scan_ipc(BytesIO(payload))
        self.push(frame, **kwargs)

