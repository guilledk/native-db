import uuid
import shutil
import warnings

from io import BytesIO
from itertools import count

from pathlib import Path
from logging import Logger, getLogger
from typing import TYPE_CHECKING

import msgspec
import polars as pl

from native_db._utils import NativeDBWarning

if TYPE_CHECKING:
    from native_db.table import Table


log = getLogger(__name__)


class TableWriterOptions(msgspec.Struct, frozen=True):
    # number of rows in staging before triggering commit
    commit_threshold: int = 400_000
    # number of rows per file, post partitioning
    rows_per_file: int = 100_000
    # set frame index sequence start
    start_frame_index: int = 0

    def __post_init__(self) -> None:
        if self.commit_threshold % self.rows_per_file != 0:
            warnings.warn(
                "TableWriterOptions's commit_threshold"
                f'({self.commit_threshold}) is not divisible by rows_per_file '
                f'({self.rows_per_file}); commits may need to leave rows on '
                'staging area until next commit or finish is called.',
                NativeDBWarning,
                stacklevel=2,
            )


class StagedFrame(msgspec.Struct):
    index: int
    number_of_rows: int
    path: Path

    def scan(self) -> pl.LazyFrame:
        return pl.scan_ipc(self.path)


class TableWriter:
    '''
    # Overview

    Helper for stream-writing tables larger than memory while respecting the
    semantics emposed by the hint system (check `native_db.dtypes` module
    docstring for information on hint semantics).

    Only a single writer is allowed per table directory.

    IMPORTANT: Don't call `.push/push_ipc` from different threads.

    # Staging vs Final

    Before doing any processing, partitioning or ordering the first step is to
    store any received frame in the "staging area" which is a subdirectory on
    the table's local path.

    The writer will keep an in memory representation of the frames in staging
    area, and once reaching a threshold it will perform all operations needed
    to join the frames respecting the table semantics specified through the
    hint system.

    Then the frame has to be moved to the "final area" for long term storage,
    which might also involve additional operations in order to arrange the
    previous frames in final area to include this new frame, keeping all
    hint semantics which are crucial for query speed later.

    ## Staging area files

    The name of the file will encode information like the number of rows and
    frame index.

    Example:

        `table_path/.staging/frame-0001.10000-rows.ipc`

        frame index: 1
        number of rows: 10k

    '''

    def __init__(
        self,
        table: 'Table',
        *,
        options: TableWriterOptions = TableWriterOptions(),
        log: Logger = log,
    ):
        self.log = log
        self.options = options

        self._table = table
        self._part = table.partitioning

        # staging state tracking

        # on disk path
        self._staging_path = table.local_path / '.staging'
        # in-memory representation of current staging area
        self._staging: list[StagedFrame] = []
        # rows stored in staging
        self._staged_rows: int = 0

        # when push is gonna be called with in order frames, an autogenerated
        # frame index is used provided by itertools.count
        self._frame_index: count[int] = count(options.start_frame_index)

        # number of rows to store in staging area before a commit
        self._commit_threshold: int = options.commit_threshold

        # rows already committed as full parts
        self._commited_rows: int = self._discover_committed_rows()

        # next parquet part index in the final dir
        self._next_part_index: int = self._discover_next_part_index()

        # next expected frame index for gap-aware commit
        self._next_expected_index: int = options.start_frame_index

    def _parquet_parts(self) -> tuple[Path, ...]:
        if not self._table.local_path.is_dir():
            return tuple()

        return tuple(
            sorted(
                self._table.local_path.rglob(
                    f'**/{self._table.prefix}-part-*.{self._table.suffix}'
                )
            )
        )

    def _discover_next_part_index(self) -> int:
        '''
        Scan the final directory and return the next 'part-xxxxx.parquet' index
        to use. If no files are present, return 0.

        '''
        max_idx = -1
        for p in self._parquet_parts():
            name = p.name  # e.g. 'part-00012.parquet'
            try:
                idx_str = name.split('-')[1].split('.')[0]
                idx = int(idx_str)
            except Exception:
                continue

            if idx > max_idx:
                max_idx = idx

        return max_idx + 1

    def _discover_committed_rows(self) -> int:
        '''
        Count how many full parts are present anywhere under final dir. Each
        part has exactly rows_per_file rows.

        '''
        if not self._table.local_path.is_dir():
            return 0

        return len(self._parquet_parts()) * self.options.rows_per_file

    def is_commit_required(self) -> bool:
        return self._staged_rows >= self._commit_threshold

    def _maybe_commit(self) -> None:
        if not self._staging or not self.is_commit_required():
            return

        staged = sorted(self._staging, key=lambda s: s.index)
        expected = self._next_expected_index
        if not staged or staged[0].index != expected:
            return

        prefix = []
        for sf in staged:
            if sf.index != expected:
                break
            prefix.append(sf)
            expected += 1
        if not prefix:
            return

        rows_per_file = self.options.rows_per_file
        total_rows = sum(sf.number_of_rows for sf in prefix)
        if total_rows < rows_per_file:
            return

        lf = pl.concat([sf.scan() for sf in prefix], rechunk=False)
        lf = self._part.prepare(lf)

        # 1) sink to temporary commit root
        commit_root = self._staging_path / f'commit-{uuid.uuid4().hex}'
        lf.sink_parquet(
            pl.PartitionParted(
                commit_root,
                by=self._part.by_cols,
                include_key=True,
                per_partition_sort_by=self._part.plcol,
            ),
            compression=self._table.compression,
            compression_level=self._table.compression_level,
            row_group_size=self._table.schema.row_group_size,
            mkdir=True,
        )

        # 2) integrate commit_root into final
        final_root = self._table.local_path
        for bucket_dir in commit_root.iterdir():
            if not bucket_dir.is_dir():
                continue
            final_bucket = final_root / bucket_dir.name
            final_bucket.mkdir(parents=True, exist_ok=True)

            # find next local index
            existing = sorted(final_bucket.glob('part-*.parquet'))
            next_idx = len(existing)

            for src in sorted(bucket_dir.glob('*.parquet')):
                dst = final_bucket / f'part-{next_idx:05d}.parquet'
                src.replace(dst)
                next_idx += 1

        shutil.rmtree(commit_root, ignore_errors=True)

        # 3) drop committed staging frames
        for sf in prefix:
            sf.path.unlink(missing_ok=True)
            self._staging.remove(sf)
            self._staged_rows -= sf.number_of_rows
            self._commited_rows += sf.number_of_rows
            self._next_expected_index = sf.index + 1

        if self.is_commit_required():
            self._maybe_commit()

    def push(
        self,
        frame: pl.LazyFrame | pl.DataFrame,
        *,
        frame_index: int | None = None,
        number_of_rows: int = 0,
    ) -> None:
        # ensure lazy frame
        if isinstance(frame, pl.DataFrame):
            frame = frame.lazy()

        # generate next frame's properties
        index = next(self._frame_index) if frame_index is None else frame_index

        if not number_of_rows:
            number_of_rows = frame.select(pl.len()).collect().item()

        path = (
            self._staging_path / f'frame-{index:04}.{number_of_rows}-rows.ipc'
        )

        # write as ipc
        frame.sink_ipc(path, compression='uncompressed', mkdir=True)

        # add to staging frame list
        self._staging.append(
            StagedFrame(index=index, number_of_rows=number_of_rows, path=path)
        )

        self._staged_rows += number_of_rows

        self._maybe_commit()

    def push_ipc(self, payload: bytes | memoryview, **kwargs) -> None:
        # to lazy frame view
        frame = pl.scan_ipc(BytesIO(payload))
        self.push(frame, **kwargs)
